# Evaluating-the-Performance-of-Episodic-Transformer-Memory-PPO-vs.-Traditional-Transformer-PPO

* Designed and implemented an Episodic Transformer Memory (ETM) framework on a Transformer-XL backbone to enhance long-term memory retention in reinforcement learning (RL) agents for partially observable environments.
* Evaluated and benchmarked ETM combined with Proximal Policy Optimization (PPO) against standard Transformer + PPO baselines, demonstrating superior learning efficiency and policy optimization in memory-intensive tasks.
* Conducted experiments to analyze the impact of persistent memory techniques on RL performance, optimizing agent decision-making over long sequences and sparse, distant observations.
* Leveraged attention-based architectures like Transformers to address limitations in traditional RL methods, enabling robust and scalable policy optimization for complex, partially observable environments.
